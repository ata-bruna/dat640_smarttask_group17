# Final project DAT640 - Semantic Answer Type prediction task (Group17)

This repository contains the files for the project entitled "SeMantic AnsweR Type prediction task"

## Table of contents


* [General Info](#general-information)
* [Requirements](#requiements)
* [Data](#data)
* [How to navigate the repository](#how-to-navigate-the-repository)


## General Information

Given a query formulated in natural language, the aim is to predict the expected answer type from a set of candidate entitites from collected target ontology. 
This project uses target ontology from DBpedia 2016 dump.


## Requirements

This project requires Python >= 3.7, Elasticsearch == 7.17.6. You must have a running local Elasticsearch instance on your machine.

```
pip install --upgrade numpy pandas scikit-learn nltk elasticsearch==7.17.6
```

## Data 

The data must be downloaded separately due to its overall size.

- [short_abstracts_en.ttl](http://downloads.dbpedia.org/2016-10/core/short_abstracts_en.ttl.bz2)</br>
- [intance_types_en.ttl](http://downloads.dbpedia.org/2016-10/core/instance_types_en.ttl.bz2)</br>
- [smart_dataset_questions](https://github.com/smart-task/smart-dataset/tree/master/datasets/DBpedia)</br>
- [dbpedia_types.tsv](https://github.com/smart-task/smart-dataset/tree/master/evaluation/dbpedia)</br>

### Project structure

The project is organized as follows:

``` sh
ğŸ“¦dat640_smarttask_group17
 â”£ ğŸ“‚datasets
 â”ƒ â”£ ğŸ“‚DBpedia
 â”ƒ â”ƒ â”£ ğŸ“œsmarttask_dbpedia_test.json  *
 â”ƒ â”— â”— ğŸ“œsmarttask_dbpedia_train.json *
 â”£ ğŸ“‚evaluation
 â”ƒ â”£ ğŸ“‚dbpedia
 â”ƒ â”— â”—ğŸ“œdbpedia_types.tsv *
 â”£ ğŸ“‚mappings
 â”ƒ â”£ ğŸ“œinstance_types_en.ttl  *
 â”ƒ â”— ğŸ“œshort_abstracts_en.ttl *
 â”£ ğŸ“‚results
 â”ƒ â”£ ğŸ“œadvanced_es_system_output.json
 â”ƒ â”£ ğŸ“œbm25_es_system_output.json
 â”ƒ â”£ ğŸ“œtest_queries_svm_output.json
 â”ƒ â”— ğŸ“œtest_type_predictions.csv
 â”£ ğŸ“œ.gitignore
 â”£ ğŸ“œadvanced_model.py
 â”£ ğŸ“œbaseline_category_prediction.py
 â”£ ğŸ“œbm25_model.py
 â”£ ğŸ“œdata_cleaning.py 
 â”£ ğŸ“œevaluate.py
 â”£ ğŸ“œindexer.ipynb
 â”£ ğŸ“œmain.py
 â”£ ğŸ“œREADME.md
 â”— ğŸ“œSVM.py

```

## How to navigate the repository

Before running the models it is necessary to create the Elasticsearch index. Run the `indexer.ipynb` notebook to do so. 
Run the file `main.py` to perform category and type prediction. Alternatively, the files `advanced_model.py` and `b25_model.py` can be run separately for type prediction. The description of the remaining files is provided below.

-  `data_cleaning.py` contains helper functions used to perform text preprocessing or to convert the data into a specific format used by the models.
-  `SVM.py` contains the pipeline used to perform category prediction.
-  `baseline_category_prediction.py` contains all algorithms tested to perform category prediction
-  `evaluate.py` contains the evaluation metrics implementation. This function was implemented by [Krisztian Balog](https://github.com/smart-task/smart-dataset/blob/master/evaluation/dbpedia/evaluate.py). The original script was kept, and the `eval_res` function was added at the end to perform the evaluation of results.

